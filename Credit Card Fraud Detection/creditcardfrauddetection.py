# -*- coding: utf-8 -*-
"""CreditCardFraudDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12RkPCVflj0mcuJWhcXCovdlLbl4DFg6Y

Importing Libraries
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#importing dataset in pandas dataframe
data = pd.read_csv('/content/creditcard.csv')
data

data.head(5)

data.tail(5)

''' * V1...V20 are the features
* Amount given in USD
* Class shows whether a transaction in legit(1) ir fraudulant(0)'''

#Summary of dataset
data.info()

#checking misisng values in each column
data.isnull().sum()

#Distribution of legit & fraudulant transactions
data["Class"].value_counts()

"""Fraudulant transactions dominate

Highly Unbalanced Dataset

Cannot feed this data to ML model since more than 90% of data is in class 0, the ML model may not recognise fraudulant transaction as it will
conclude that a transaction is legit even if it is not. We should do processing

0-> Fraudulant transaction
1-> Legit transaction
"""

#seperating data
#storing legit transaction and fraudulant transactions in seperate variables
legit = data[data.Class==1]
fraud = data[data.Class==0]

print(legit.shape)
print(fraud.shape)

#statistical information of data
legit.Amount.describe()

fraud.Amount.describe()

"""Mean values of Fradulant transactions are higher than Legit transactions.

25% of the data is below the value.Similarly for 50% aand 75%
"""

#Comparing values for both transactions
data.groupby('Class').mean()

"""By checking the difference between both transactions ML algorithms can predict whether a transaction is fraudulant or legit

Under-Sampling : reducing the number of majority class data to balance an unbalanced dataset
"""

#Building a sample dataset containing similar distribution of legit/normal & fraudulant transaction

#No. of fraudulant transactions = 27725
#No. of legit transactions = 93

fraud_sample = fraud.sample(n = 93) #gives random 93 datapoints

#Concatenating two dataframes

new_data = pd.concat([fraud_sample, legit], axis=0) #joins two dataframe one after another

new_data

new_data.head(5)

new_data.tail(5)

new_data.shape

new_data['Class'].value_counts()

new_data.groupby('Class').mean() #checking if we got a good or bad sample

"""Splitting Data into features and targets(0,1)"""

X = new_data.drop(columns='Class', axis=1) #axis = 0 represents a row and 1 represents a column
Y= new_data['Class']

print(X)

print(Y)

"""Split data into Training and Testing data"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=2)
#20% go to testing data and 80% in training data
#stratify -> both classes (from Y) will be evenly distributed

"""X is features and Y is labels(class). Splitting X and Y randomly."""

print(X.shape, X_train.shape, X_test.shape)

"""Training Model. Model - Logistic Regression Model"""

model = LogisticRegression()

#training the model with training data
model.fit(X_train, Y_train) #fits data in model

"""Evaluation of Model - based on Accuracy Score"""

#accuracy of training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
#predicting the lebels for the X_train values

print("Accuracy of Training Data: ", training_data_accuracy)

#accuracy for test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print("Accuracy of Test Data: ", test_data_accuracy)

"""If accuracy of training data and test data vary significantly then the model is underfitted or overfitted"""

